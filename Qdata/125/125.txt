Since their introduction about 25 years ago, machine learning (ML) potentials have become an important tool in the field of atomistic simulations. After the initial decade, in which neural networks were successfully used to construct potentials for rather small molecular systems, the development of high-dimensional neural network potentials (HDNNPs) in 2007 opened the way for the application of ML potentials in simulations of large systems containing thousands of atoms. To date, many other types of ML potentials have been proposed continuously increasing the range of problems that can be studied. In this review, the methodology of the family of HDNNPs including new recent developments will be discussed using a classification scheme into four generations of potentials, which is also applicable to many other types of ML potentials. The first generation is formed by early neural network potentials designed for low-dimensional systems. High-dimensional neural network potentials established the second generation and are based on three key steps: first, the expression of the total energy as a sum of environment-dependent atomic energy contributions; second, the description of the atomic environments by atom-centered symmetry functions as descriptors fulfilling the requirements of rotational, translational, and permutation invariance; and third, the iterative construction of the reference electronic structure data sets by active learning. In third-generation HDNNPs, in addition, long-range interactions are included employing environment-dependent partial charges expressed by atomic neural networks. In fourth-generation HDNNPs, which are just emerging, in addition, nonlocal phenomena such as long-range charge transfer can be included. The applicability and remaining limitations of HDNNPs are discussed along with an outlook at possible future developments.